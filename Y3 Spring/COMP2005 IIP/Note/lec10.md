# lec10: Image Compression

[toc]

## Introduction

**Image Compression**

-   Individual images can be large
-   Images are easy to acquire, collections increase rapidly
-   In some applications, images are gathered automatically
-   Luckily, image data is redundant in several ways
    -   Coding redundancy
    -   Spatial redundancy
    -   Psychovisual redundancy

### Coding Redundancy

-   The grey level histogram of an image gives the probability (frequency) of occurrence of grey level $r_k$

$$
p(r_k) = \frac{n_k}{n} , \  k = 0.2, \dots, L-1
$$

-   If the number of bits used to represent each value of $r_k$ is $l(r_k)$, **the average number of bits required to represent a pixel is**

$$
L_{avg} = \sum_{k=0}^{L-1}l(r_k)p(r_k)
$$

-   To code an `M*N` image requires `M*N*L_avg` bits
-   If an m-bit natural binary code is used to represent grey level then
    -   all pixels take the same amount of space,
    -   $p(r_k)$​ values sum to 1, so
    -   and an image occupies `MNm` bits

$$
L_{avg} = \sum_{k=0}^{L-1}l(r_k)p(r_k) = \sum_{k=0}^{L-1}mp(r_k) = m
$$

-   But some pixel values are more common than others…..

**Variable Length Encoding**

-   Assigning fewer bits to the more probable grey levels than to less probable ones can achieve data compression, e.g:

<img src="assets/Screenshot 2024-05-24 at 22.54.00.png" alt="Screenshot 2024-05-24 at 22.54.00" style="zoom:50%;" />

-   Build a codebook, replace ‘true’ pixel values with code
-   Lossless: the process can be reversed by inverting the codebook

### Spatial Redundancy

-   Sometimes called **Interpixel Redundancy**
-   Neighbouring pixels often have similar values
-   Compression based on spatial redundancy involves some element of pixel grouping, or transformation
-   Simplest is Run-Length Encoding
    -   maps the pixels along each scan line into a sequence of pairs (g1 , r1 ), (g2 , r2 ), …,
    -   where g i is the ith grey level, r i is the run length of ith run

**A binary example**

<img src="assets/Screenshot 2024-05-24 at 22.56.51.png" alt="Screenshot 2024-05-24 at 22.56.51" style="zoom:50%;" />

### Psychovisual Redundancy

-   Some grey level and colour differences are imperceptible; goal is to compress without noticeable change to the image

<img src="assets/Screenshot 2024-05-24 at 23.02.44.png" alt="Screenshot 2024-05-24 at 23.02.44" style="zoom:50%;" />

**Evaluating Compression**

-   Fidelity Criteria: success is judged by comparing original and compressed versions
-   Some measures are objective, e.g. root mean square error (erms ) and signal to noise ratio (SNR)
-   Let f(x,y) be the input image, f’(x,y) be reconstructed input image from compressed bit stream, then

<img src="assets/Screenshot 2024-05-24 at 23.03.27.png" alt="Screenshot 2024-05-24 at 23.03.27" style="zoom:50%;" />

**Fidelity Criteria**

-   `e_rms` and SNR are convenient objective measures
-   Most decompressed images are viewed by human beings
-   Subjective evaluation of compressed image quality by human observers are often more appropriate

### Image Compression Systems

-   Mapper (spatial redundancy)
    -   transforms input data in a way that facilitates reduction of interpixel redundancies
    -   reversible
-   Quantiser (psychvisual redundancy)
    -   transforms input data in a way that facilitates reduction of psychovisual redundancies
    -   **not reversible**
-   Symbol coder (coding redundancy)
    -   assigns the shortest code to the most frequently occurring output values
    -   reversible

<img src="assets/Screenshot 2024-05-24 at 23.05.20.png" alt="Screenshot 2024-05-24 at 23.05.20" style="zoom:50%;" />

## Image Compression: Huffman Coding

**Exploiting Coding Redundancy**

-   These methods are derived from information theory: try to maintain a high level of information in compressed images
-   Not limited to images, are applicable to any digital information.
    -   speak of symbols instead of pixel values and sources instead of images
    -   exploit nonuniform probabilities of symbols (nonuniform histograms) and use a variable-length code.
-   Evaluation requires a measure of the information content of a source: **Entropy**

### Entropy 熵

-   The idea: associate information with probability
-   A random event E with probability P(E) contains the following number of  units of information

$$
I(E) = \log (\frac{1}{P(E)}) = - \log(P(E))
$$

-   Suppose that grey level values are generated by a random variable, then r k contains:

$$
I(r_k) = -\log (P(r_k))
$$

-   Entropy is the average information content of an image, a measure of histogram dispersion

<img src="assets/Screenshot 2024-05-24 at 23.08.48.png" alt="Screenshot 2024-05-24 at 23.08.48" style="zoom:50%;" />

-   Can’t compress to less than H bits/pixel without losing information

### Huffman Coding 

>   [b站视频链接](https://www.bilibili.com/video/BV18V411v7px/?spm_id_from=333.337.search-card.all.click&vd_source=acc215f6914b68c7f4072a5f3e332d05)

-   Compute probabilities of each symbol by histogramming the source
-   Process probabilities to pre-compute codebook: code(i)
    -   codebook is static (fixed)
-   Encode source symbol-by-symbol: symbol(i) -> code(i)
-   Transmit coded signal and codebook
-   The need to pre-process (histogram) the source before encoding begins is a disadvantage
-   Builds a binary tree in which symbols to be coded are nodes

**The algorithm:**

-   Create a list of nodes, one per for symbol, **sorted in order** of symbol frequency (or probability)
-   REPEAT (until only one node left)
    -   Pick the two nodes with the **lowest frequencies**/probabilities and **create a parent** of them
    -   **Randomly** assign the codes 0,1 to the two new branches of the tree and **delete the children from the list**
    -   **Assign the sum** of the children’s probabilities to their parent and **insert it** in the list
-   Path from root to node gives code for corresponding symbol

<img src="assets/Screenshot 2024-05-24 at 23.20.04.png" alt="Screenshot 2024-05-24 at 23.20.04" style="zoom:50%;" />

<img src="assets/Screenshot 2024-05-24 at 23.20.14.png" alt="Screenshot 2024-05-24 at 23.20.14" style="zoom:50%;" />

-   The algorithm systematically places nodes representing high probability symbols further up the tree: their paths (and so codes) are shorter
-   **No code is a prefix to any other** – don’t need to mark boundaries between codes
    -   e.g. 01101010 must be a1 a3 
-   In this example
    -   Average length of the code is 2.2.bits/symbol
    -   The entropy of the source is 2.14 bits/symbol
-   In general
    -   Break image into small (e.g. 8 x 8) blocks
    -   Each block is a symbol to be encoded

**An Example**

<img src="assets/Screenshot 2024-05-24 at 23.21.10.png" alt="Screenshot 2024-05-24 at 23.21.10" style="zoom:50%;" />

## Image Compression: GIF

**Using Psychovisual Redundancy**

-   Quantisation is widely used
    -   Represent areas of grey level/colour space with fewer bits
    -   Lossy: cannot be inverted
    -   Find the best tradeoff between
-   Scalar Quantisation (i.e. quantising scalar values)

<img src="assets/Screenshot 2024-05-24 at 23.30.14.png" alt="Screenshot 2024-05-24 at 23.30.14" style="zoom:50%;" />

**Vector Quantisation**

-   Palettised images (gif)
    -   Map vector values (R,G,B) onto scalar values
    -   Multiple vectors map to each scalar

<img src="assets/Screenshot 2024-05-24 at 23.30.48.png" alt="Screenshot 2024-05-24 at 23.30.48" style="zoom:50%;" />

**Paletised Images**

<img src="assets/Screenshot 2024-05-24 at 23.36.29.png" alt="Screenshot 2024-05-24 at 23.36.29" style="zoom:50%;" />

**Building a Palette**

<img src="assets/Screenshot 2024-05-24 at 23.37.23.png" alt="Screenshot 2024-05-24 at 23.37.23" style="zoom:50%;" />

<img src="assets/Screenshot 2024-05-24 at 23.37.37.png" alt="Screenshot 2024-05-24 at 23.37.37" style="zoom:50%;" />

<img src="assets/Screenshot 2024-05-24 at 23.38.51.png" alt="Screenshot 2024-05-24 at 23.38.51" style="zoom:50%;" />

## Image Compression: JPEG

**Exploiting Spatial Redundancy**

-   Run-length encoding needs adjacent pixels to be **equal**
-   Pixels are more often **highly correlated (dependent)**
    -   Not equal, but can predict the image pixels to be coded from those already coded
-   Each pixel value (except at the boundaries) is predicted based on its neighbors (e.g., linear combination) to get a predicted image.
-   The difference between the original and predicted images yields a differential or residual image with a reduced set of values.
-   The differential image is encoded using Huffman coding, or similar.

**Differential Pulse-code Modulation**

-   Code the difference between adjacent pixels

<img src="assets/Screenshot 2024-05-24 at 23.40.09.png" alt="Screenshot 2024-05-24 at 23.40.09" style="zoom:50%;" />

-   Prediction is that the next pixel value = current one
-   Need the first value to provide a point of reference
-   Invertible (lossless) and lower entropy

<img src="assets/Screenshot 2024-05-24 at 23.40.38.png" alt="Screenshot 2024-05-24 at 23.40.38" style="zoom:50%;" />

**Predictive Coding**

-   Higher order pattern prediction
-   Use both 1D and 2D patterns (to predict shaded pixel)

<img src="assets/Screenshot 2024-05-24 at 23.41.07.png" alt="Screenshot 2024-05-24 at 23.41.07" style="zoom:50%;" />

**A Complete System: JPEG**

-   A set of methods with a common baseline system
    -   Discrete Cosine Transform
    -   Quantisation
    -   Variable length encoding
-   A JPEG-compatible product must only include support for the baseline
-   Increasing the amount of quantisation reduces file size but introduces artefacts: blocks become visible

<img src="assets/Screenshot 2024-05-24 at 23.41.49.png" alt="Screenshot 2024-05-24 at 23.41.49" style="zoom:50%;" />